{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b17f29-f7a7-42b3-9b87-1ad7b001c06c",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fed63d-da04-4569-ac5c-622e112bf4f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'storageforreview'\n",
    "file_name = 'gs://storageforreview/company_review.csv'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "print(\"Data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc067d-20b4-4ea0-ac79-da3d4caa9bc3",
   "metadata": {},
   "source": [
    "Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68428f46-22c6-4b6c-8e7d-35e1b79a5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "# Print the column names of the dataset\n",
    "print(df.columns)\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884c0ad-9376-4924-ba6b-1aabf41a900d",
   "metadata": {},
   "source": [
    "Explore Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263e2ef-ad13-46f3-b315-412407313bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6aab02-519c-4c17-a878-2e2afbfa5e6f",
   "metadata": {},
   "source": [
    "Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a55635-90b8-400b-bb68-339736e0dd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Example: Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(\"Data cleaned successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181c33e-6085-423d-8d88-aa26f1cf4d01",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daf83e-d0ec-47c2-a639-059f77ba1004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Extract text and sentiment columns\n",
    "texts = df['reviewText']  \n",
    "labels = df['overall'] \n",
    "\n",
    "# Convert ratings to sentiment labels\n",
    "# Assuming a rating of 4 or 5 is positive sentiment, and 1 to 3 is negative sentiment\n",
    "sentiment_map = {1: 'negative', 2: 'negative', 3: 'negative', 4: 'positive', 5: 'positive'}\n",
    "df['sentiment'] = df['overall'].map(sentiment_map)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['reviewText'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')  # Out-Of-Vocabulary token\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Define the maximum length for padding\n",
    "max_length = max(max(len(seq) for seq in X_train_sequences), max(len(seq) for seq in X_val_sequences))\n",
    "\n",
    "# Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(f\"Training data shape: {X_train_padded.shape}, Validation data shape: {X_val_padded.shape}\")\n",
    "\n",
    "print(\"Data preprocessed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118059d-8134-44ef-b5fa-8192fe3682b0",
   "metadata": {},
   "source": [
    " Train and Evaluate the Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825ca59-6494-4a00-899a-1e64e81b3bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Extract text and sentiment columns\n",
    "texts = df['reviewText']\n",
    "labels = df['overall']\n",
    "\n",
    "# Convert ratings to sentiment labels\n",
    "# Assuming a rating of 4 or 5 is positive sentiment, and 1 to 3 is negative sentiment\n",
    "sentiment_map = {1: 'negative', 2: 'negative', 3: 'negative', 4: 'positive', 5: 'positive'}\n",
    "df['sentiment'] = df['overall'].map(sentiment_map)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "maxlen = 1579\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print first few rows to verify\n",
    "print(df.head())\n",
    "print(\"Data preprocessed successfully\")\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('sentiment_model.keras')\n",
    "print(\"Model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5112f8-a1df-43a6-9ac3-c25d937adc8e",
   "metadata": {},
   "source": [
    "Upload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6333-def3-4fed-ba0f-97cef067c143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Initialize the GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = 'storageforreview'\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Define paths\n",
    "local_model_path = 'sentiment_model.keras'\n",
    "saved_model_path = 'sentiment_model_saved'\n",
    "blob_path = 'models/sentiment_model_saved'\n",
    "\n",
    "# Upload the Keras model file to GCS\n",
    "blob = bucket.blob(f'models/{local_model_path}')\n",
    "blob.upload_from_filename(local_model_path)\n",
    "print(\"Keras model uploaded to GCS successfully.\")\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(local_model_path)\n",
    "print(\"Keras model loaded.\")\n",
    "\n",
    "# Save as a SavedModel format\n",
    "model.save(saved_model_path)\n",
    "print(\"Model converted to SavedModel format and saved.\")\n",
    "\n",
    "# Upload the SavedModel to GCS\n",
    "for root, dirs, files in os.walk(saved_model_path):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_file_path, saved_model_path)\n",
    "        blob = bucket.blob(f'{blob_path}/{relative_path}')\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "        print(f'Uploaded {local_file_path} to {blob.path}.')\n",
    "\n",
    "print(\"SavedModel uploaded to GCS successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33202364-1b14-4c0e-88e3-4f4f9b53dd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI client\n",
    "aiplatform.init(project='sentimentanalysis-429522', location='us-central1')\n",
    "\n",
    "# List all models\n",
    "models = aiplatform.Model.list()\n",
    "\n",
    "# Print model details\n",
    "for model in models:\n",
    "    print(f\"Model ID: {model.name}, Model Display Name: {model.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f073ac-2065-4aa0-8a0e-01f01a95ff8c",
   "metadata": {},
   "source": [
    "Create end points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa917d64-8b1f-436c-9f87-0b9688e797d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI client\n",
    "aiplatform.init(project='sentimentanalysis-429522', location='us-central1')\n",
    "\n",
    "# Create an endpoint\n",
    "endpoint = aiplatform.Endpoint.create(display_name='sentiment-analysis-endpoint')\n",
    "print(f\"Endpoint created: {endpoint.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5004d09-0d3b-4c45-b1ba-b8b045acf157",
   "metadata": {},
   "source": [
    "Deploy the model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0d5aa-7b8d-4346-9673-9c188d284dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the model to the endpoint\n",
    "model = aiplatform.Model('7473346449133010944')\n",
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type='n1-standard-4',  # Adjust machine type as needed\n",
    ")\n",
    "print(f\"Model deployed to endpoint: {deployed_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ea3ed-6c07-4815-9b1d-83f024135f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "Prepare a sample request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ec6f1-ba14-443e-9712-9823a0066661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI Endpoin\n",
    "#endpoint = aiplatform.Endpoint(endpoint_name=\"YOUR_ENDPOINT_NAME\")\n",
    "\n",
    "# Define the instances with the expected input format\n",
    "instances = [\n",
    "    {'reviewText': 'The product is excellent and I am very satisfied.'}\n",
    "]\n",
    "\n",
    "# Make a prediction request\n",
    "try:\n",
    "    predictions = endpoint.predict(instances=instances)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f2438-c8cd-48f8-88f1-6f6bcfb01259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
